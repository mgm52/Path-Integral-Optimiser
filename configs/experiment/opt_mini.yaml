# @package _global_

# verify the loss curve:
# python run.py experiment=ou.yaml logger=wandb

defaults:
  - override /mode: default.yaml
  - override /trainer: default.yaml
  - override /model: base.yaml
  - override /datamodule: opt_mini.yaml
  - override /callbacks: null
  - override /logger: null

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "opt_mini"

seed: 12345
print_config: false

trainer:
  min_epochs: 1
  max_epochs: -1
  max_steps: -1
  log_every_n_steps: 1
  # default 0.0 (off)
  gradient_clip_val: 0.0
  # default "norm", alternatively "value"
  gradient_clip_algorithm: "norm"

  weights_summary: null
  resume_from_checkpoint: null

model:
  lr: 0.01
  # This is the batch size in the task
  batch_size: 100
  sde_model:
    f_format: "f"
  optimizer: "pis"
  sampling_repeats_per_data: 8

datamodule:
  dataset:
    sigma: 0.01
  dl:
    # This will be the no. of trajectories
    # In theory: higher==lower variance, learn faster
    batch_size: 32

callbacks:
  #sample:
    #_target_: src.callbacks.opt_mini_cb.OptMiniSample
    #every_n: 25
  lr:
    _target_: src.callbacks.lr.LinearScheduler

logger:
  wandb:
    name: null #pis-${name}
