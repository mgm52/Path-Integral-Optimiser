# @package _global_

# verify the loss curve:
# python run.py experiment=ps.yaml logger=wandb
# python run.py experiment=ps.yaml callbacks.sample.every_n=1

defaults:
  - override /mode: default.yaml
  - override /trainer: default.yaml
  - override /model: base.yaml
  - override /datamodule: funnel
  - override /callbacks: null
  - override /logger: null

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "funnel"

seed: 12345

trainer:
  min_epochs: 1
  max_epochs: 100
  max_steps: 1500
  log_every_n_steps: 10
  flush_logs_every_n_steps: 10

callbacks:
  sample:
    _target_: src.callbacks.metric_cb.VizSampleDist
    every_n: 25
  lr:
    _target_: src.callbacks.lr.LinearScheduler
  lr_m:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: epoch
